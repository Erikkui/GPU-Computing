{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM40A1401 GPU Computing\n",
    "\n",
    "## Erik Kuitunen\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Implement vector differentiation similar to numpy diff function using shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "  __global__ void vec_diff_sharedmem( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int tId = threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "    \n",
    "    __shared__ float a_shared[ %(THREADS)s ];\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "      \n",
    "    // Each thread loads one element from global to shared mem\n",
    "    a_shared[ tId ] = a[ index ];\n",
    "     \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Handling the case, where calulation happens on the edge of a block or at the end of the data\n",
    "    if ( tId == blockDim.x - 1 || index  == data_size - 2 ) {   \n",
    "    \n",
    "      float edge = a[ index + 1 ];\n",
    "      \n",
    "      b_elem = edge - a_shared[ tId ];\n",
    "    \n",
    "    } else {\n",
    "      \n",
    "      b_elem = a_shared[ tId + 1 ] - a_shared[ tId ];\n",
    "      \n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  } \n",
    "  \n",
    "  // Also non-shared memory version for comparison\n",
    "  \n",
    "  __global__ void vec_diff( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "    \n",
    "    b_elem = a[ index + 1 ] - a[ index ];\n",
    "      \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  }\n",
    "\"\"\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data vectors and initalize thread and block sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 16*16\n",
    "\n",
    "data_dim = 2**28\n",
    "\n",
    "a = np.float32( np.random.rand( 1, data_dim ) )\n",
    "\n",
    "grid_dim = math.ceil( data_dim / THREADS ) \n",
    "\n",
    "block_dims = ( THREADS, 1, 1 )\n",
    "grid_dims = ( grid_dim, 1, 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate memory and copy data from host to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_gpu = cuda.mem_alloc( a.nbytes )\n",
    "cuda.memcpy_htod( a_gpu, a )\n",
    "\n",
    "b = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu = cuda.mem_alloc( b.nbytes )\n",
    "\n",
    "b_noshared = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu_noshared = cuda.mem_alloc( b_noshared.nbytes )\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the CUDA kernel and copy the result back to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying constant THREADS for shared memory kernel\n",
    "kernel = kernel_code % {\n",
    "        'THREADS': THREADS\n",
    "        }\n",
    "\n",
    "# Compile the shared memory kernel code\n",
    "mod = SourceModule( kernel )\n",
    "\n",
    "diff_gpu = mod.get_function( \"vec_diff_sharedmem\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu( a_gpu, b_gpu, np.int32( data_dim ), \n",
    "        block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b, b_gpu )\n",
    "\n",
    "###### Doing the same as above for non-shared memory version\n",
    "diff_gpu_noshared = mod.get_function( \"vec_diff\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu_noshared( a_gpu, b_gpu_noshared, np.int32( data_dim ), \n",
    "                block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time_noshared = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b_noshared, b_gpu_noshared )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors are the same. \n",
      "CPU: 178.29549999441952 ms\n",
      "GPU, no shared memory: 0.12549999519251287 ms\n",
      "GPU, shared memory: 0.34339999547228217 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure cpu time\n",
    "cpu_start = timeit.default_timer()  \n",
    " \n",
    "b_cpu = np.diff( a )\n",
    "\n",
    "cpu_time = ( timeit.default_timer() - cpu_start ) * 1000\n",
    "\n",
    "if ( b_cpu == b ).all() and (b_cpu == b_noshared ).all():\n",
    "    print( \"The vectors are the same. \\nCPU: \" + str( cpu_time )+ \" ms\\nGPU, no shared memory: \" \n",
    "            + str( gpu_time_noshared ) + \" ms\\nGPU, shared memory: \" + str( gpu_time ) + \" ms\")   \n",
    "else:\n",
    "    print( \"The vector are not the same. Something is wrong.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory version is slower. Bug in code, or is this kind of problem just generally more inefficient with shared memory? On the other hand, if running no shared memory first the shared memory version last, shared is (sometimes) faster. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Implement the three reduction models presented in the lectures. Time their performance against different vector sizes. Execution times can vary between executions, so run them for example 100 times and take the average time.\n",
    "\n",
    "Add also CPU performance with numpy sum() - function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = np.array( [ 2**10, 2**13, 2**16, 2**20, 2**22 ] )\n",
    "\n",
    "exec_times_gpu = np.zeros( ( 4, np.size( data_sizes ) ) )\n",
    "exec_times_cpu = np.empty_like( exec_times_gpu )\n",
    "\n",
    "threads_per_block = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "               \n",
    "  __global__ void interleaved( const float*, int stride ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int tId = threadIdx.x;\n",
    "    \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void sequential( const float* a, int stride ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void interleaved_shared( const float* a, int stride ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void sequential_shared( const float* a, int stride ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like() missing 1 required positional argument: 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m jj \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m data_sizes:    \n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Specifying thread and block dimensions for kernel call\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     block_dims \u001b[38;5;241m=\u001b[39m [ threads_per_block, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros_like() missing 1 required positional argument: 'a'"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_kernel = SourceModule( kernel_code ).get_function( 'interleaved' )\n",
    "\n",
    "# Looping through data sizes, doig the calculations and saving results.\n",
    "ii = 0\n",
    "jj = 0\n",
    "for size in data_sizes:    \n",
    "    \n",
    "    result = np.zeros_like( )\n",
    "    \n",
    "    # Specifying thread and block dimensions for kernel call\n",
    "    block_dims = [ threads_per_block, 1, 1 ]\n",
    "    grid_dims = [ math.ceil( size / threads_per_block), 1, 1 ]\n",
    "    \n",
    "    # Create data and allocate memory\n",
    "    a = np.random.randn( 1, size ).astype( np.float32 )\n",
    "    a_gpu = cuda.mem_alloc( a.nbytes )\n",
    "    \n",
    "    cuda.memcpy_htod( a_gpu, a )\n",
    "    \n",
    "    sum_kernel( a, block = block_dims, grid = grid_dims )\n",
    "    \n",
    "    cuda.memcpy_dtoh( result, a_gpu )\n",
    "    \n",
    "    exec_times_gpu[ii][jj] = result[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "\n",
    "Implement Sequential reduction with shared memory. Add the performance to the same plot used in the previous task.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
