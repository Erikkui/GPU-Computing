{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM40A1401 GPU Computing\n",
    "\n",
    "## Erik Kuitunen\n",
    "\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Implement vector differentiation similar to numpy diff function using shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "  __global__ void vec_diff_sharedmem( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "    \n",
    "    __shared__ float a_sub1[ %(THREADS)s ];\n",
    "    __shared__ float a_sub2[ %(THREADS)s ];\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "      \n",
    "    a_sub1[ threadIdx.y ] = a[ index ];\n",
    "    a_sub2[ threadIdx.y ] = a[ index + 1 ];  \n",
    "    \n",
    "    b_elem = a_sub2[ threadIdx.y ] - a_sub1[ threadIdx.y ];\n",
    "      \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  } \n",
    "  \n",
    "  // Also non-shared memory version for comparison\n",
    "  \n",
    "  __global__ void vec_diff( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "    \n",
    "    b_elem = a[ index + 1 ] - a[ index ];\n",
    "      \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  }\n",
    "\"\"\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data vectors and initalize thread and block sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 16*16\n",
    "\n",
    "data_dim = 2**27\n",
    "\n",
    "a = np.float32( np.random.rand( 1, data_dim ) )\n",
    "\n",
    "grid_dim = math.ceil( data_dim / THREADS ) \n",
    "\n",
    "block_dims = ( THREADS, 1, 1 )\n",
    "grid_dims = ( grid_dim, 1, 1 )\n",
    "size_shared = a[0].nbytes * THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate memory and copy data from host to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_gpu = cuda.mem_alloc( a.nbytes )\n",
    "cuda.memcpy_htod( a_gpu, a )\n",
    "\n",
    "b = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu = cuda.mem_alloc( b.nbytes )\n",
    "\n",
    "b_noshared = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu_noshared = cuda.mem_alloc( b_noshared.nbytes )\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the CUDA kernel and copy the result back to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikk\\AppData\\Local\\Temp\\ipykernel_18932\\2989837996.py:7: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\n",
      "\n",
      "  mod = SourceModule( kernel )\n"
     ]
    }
   ],
   "source": [
    "# Specifying constant THREADS for shared memory kernel\n",
    "kernel = kernel_code % {\n",
    "        'THREADS': THREADS\n",
    "        }\n",
    "\n",
    "# Compile the shared memory kernel code\n",
    "mod = SourceModule( kernel )\n",
    "diff_gpu = mod.get_function( \"vec_diff_sharedmem\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu( a_gpu, b_gpu, np.int32( data_dim ), \n",
    "        block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b, b_gpu )\n",
    "\n",
    "###### Doing the same as above for non-shared memory version\n",
    "diff_gpu_noshared = mod.get_function( \"vec_diff\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu_noshared( a_gpu, b_gpu_noshared, np.int32( data_dim ), \n",
    "                block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time_noshared = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b_noshared, b_gpu_noshared )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors are the same. \n",
      "CPU: 93.51569999853382 ms\n",
      "GPU, no shared memory: 0.12149999929533806 ms\n",
      "GPU, shared memory: 0.1326000001427019 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure cpu time\n",
    "cpu_start = timeit.default_timer()  \n",
    " \n",
    "b_cpu = np.diff( a )\n",
    "\n",
    "cpu_time = ( timeit.default_timer() - cpu_start ) * 1000\n",
    "\n",
    "if ( b_cpu == b ).all() and (b_cpu == b_noshared ).all():\n",
    "    print( \"The vectors are the same. \\nCPU: \" + str( cpu_time )+ \" ms\\nGPU, no shared memory: \" \n",
    "            + str( gpu_time_noshared ) + \" ms\\nGPU, shared memory: \" + str( gpu_time ) + \" ms\")   \n",
    "else:\n",
    "    print( \"The vector are not the same. Something is wrong.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory version is slower. Bug in code, or is this kind of problem just generally more inefficient with shared memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Implement the three reduction models presented in the lectures. Time their performance against different vector sizes. Execution times can vary between executions, so run them for example 100 times and take the average time.\n",
    "\n",
    "Add also CPU performance with numpy sum() - function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = np.array( [ 2**10, 2**13, 2**16, 2**20, 2**22 ] )\n",
    "exec_times_gpu = []\n",
    "exec_times_cpu = []\n",
    "\n",
    "threads_per_block = 1024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Looping through data sizes, doig the calculations and saving results.\n",
    "for size in data_sizes:     \n",
    "    \n",
    "    blocks = math.ceil( size / threads_per_block)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "  __global__ void vec_diff_sharedmem( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3:\n",
    "\n",
    "Implement Sequential reduction with shared memory. Add the performance to the same plot used in the previous task.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
