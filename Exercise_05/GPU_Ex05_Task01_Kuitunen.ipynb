{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM40A1401 GPU Computing\n",
    "\n",
    "## Erik Kuitunen\n",
    "\n",
    "### Exercise 5 Task 1\n",
    "\n",
    "Tensors are a generalisation of multidimensional arrays and the data structure in PyTorch. Experiment with PyTorch tensors as follows:\n",
    "\n",
    "1. Create a 2D tensor of size 5x3 where the elements are pulled from the normal probability distribution with zero mean and unit variance.\n",
    "2. Present the memory contents and the metadata (size, offset, stride) of the tensor.\n",
    "3. Transpose the tensor without copying in memory.\n",
    "4. Present the memory contents and the metadata (size, offset, stride) of the tensor.\n",
    "5. Test whether the tensor and its transpose are contiguous in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor T:  \n",
      " tensor([[-0.4187,  0.7139, -1.2003],\n",
      "        [ 1.1704, -0.3430,  0.2135],\n",
      "        [ 0.2866,  0.0790,  0.4731],\n",
      "        [-0.6842,  0.5557, -1.2893],\n",
      "        [-0.6096,  0.1483,  1.9090]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = torch.randn( 5, 3 )\n",
    "print( \"Tensor T: \", \"\\n\", T, \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Memory contents and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory contents of T:  \n",
      "  -0.41869619488716125\n",
      " 0.713928759098053\n",
      " -1.2003039121627808\n",
      " 1.1704343557357788\n",
      " -0.342976838350296\n",
      " 0.21345455944538116\n",
      " 0.28663983941078186\n",
      " 0.07904254645109177\n",
      " 0.47305628657341003\n",
      " -0.6842312812805176\n",
      " 0.5557144284248352\n",
      " -1.289341926574707\n",
      " -0.6095649600028992\n",
      " 0.1482715606689453\n",
      " 1.909048080444336\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 15] \n",
      "\n",
      "Metadata of T:  \n",
      " Size:  torch.Size([5, 3]) ; offset:  0 ; stride:  (3, 1) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikk\\AppData\\Local\\Temp\\ipykernel_14984\\1542109320.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print( \"Memory contents of T: \", \"\\n\", T.storage(), \"\\n\" )\n"
     ]
    }
   ],
   "source": [
    "print( \"Memory contents of T: \", \"\\n\", T.storage(), \"\\n\" )\n",
    "print( \"Metadata of T: \", \"\\n\", \"Size: \", T.size(), \"; offset: \", T.storage_offset(), \"; stride: \", T.stride(), \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Transpose the tensor without copying in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose of T:  \n",
      " tensor([[-0.4187,  1.1704,  0.2866, -0.6842, -0.6096],\n",
      "        [ 0.7139, -0.3430,  0.0790,  0.5557,  0.1483],\n",
      "        [-1.2003,  0.2135,  0.4731, -1.2893,  1.9090]])\n"
     ]
    }
   ],
   "source": [
    "T_t = T.transpose( 0, 1 )\n",
    "print( \"Transpose of T: \", \"\\n\", T_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Memory contents and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory contents of T_t:  \n",
      "  -0.41869619488716125\n",
      " 0.713928759098053\n",
      " -1.2003039121627808\n",
      " 1.1704343557357788\n",
      " -0.342976838350296\n",
      " 0.21345455944538116\n",
      " 0.28663983941078186\n",
      " 0.07904254645109177\n",
      " 0.47305628657341003\n",
      " -0.6842312812805176\n",
      " 0.5557144284248352\n",
      " -1.289341926574707\n",
      " -0.6095649600028992\n",
      " 0.1482715606689453\n",
      " 1.909048080444336\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 15] \n",
      "\n",
      "Metadata of T_t:  \n",
      " Size:  torch.Size([3, 5]) ; offset:  0 ; stride:  (1, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( \"Memory contents of T_t: \", \"\\n\", T_t.storage(), \"\\n\" )\n",
    "print( \"Metadata of T_t: \", \"\\n\", \"Size: \", T_t.size(), \"; offset: \", T_t.storage_offset(), \"; stride: \", T_t.stride(), \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check whether tensors are contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print( T.is_contiguous(), T_t.is_contiguous() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
