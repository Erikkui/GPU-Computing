{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM40A1401 GPU Computing\n",
    "\n",
    "## Erik Kuitunen\n",
    "\n",
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycuda\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Implement vector differentiation similar to numpy diff function using shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "  __global__ void vec_diff_sharedmem( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int tId = threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "    \n",
    "    __shared__ float a_shared[ %(THREADS)s ];\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "      \n",
    "    // Each thread loads one element from global to shared mem\n",
    "    a_shared[ tId ] = a[ index ];\n",
    "     \n",
    "    __syncthreads();\n",
    "    \n",
    "    // Handling the case, where calulation happens on the edge of a block or at the end of the data\n",
    "    if ( tId == blockDim.x - 1 || index  == data_size - 2 ) {   \n",
    "    \n",
    "      float edge = a[ index + 1 ];\n",
    "      \n",
    "      b_elem = edge - a_shared[ tId ];\n",
    "    \n",
    "    } else {\n",
    "      \n",
    "      b_elem = a_shared[ tId + 1 ] - a_shared[ tId ];\n",
    "      \n",
    "    }\n",
    "    \n",
    "    __syncthreads();\n",
    "    \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  } \n",
    "  \n",
    "  // Also non-shared memory version for comparison\n",
    "  \n",
    "  __global__ void vec_diff( const float* a, float* b, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float b_elem = 0;\n",
    "\n",
    "    if ( index > data_size-2 ) {\n",
    "      return;\n",
    "    }\n",
    "    \n",
    "    b_elem = a[ index + 1 ] - a[ index ];\n",
    "      \n",
    "    b[ index ] = b_elem;\n",
    "     \n",
    "  }\n",
    "\"\"\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data vectors and initalize thread and block sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS = 16*16\n",
    "\n",
    "data_dim = 2**20\n",
    "\n",
    "a = np.float32( np.random.rand( 1, data_dim ) )\n",
    "\n",
    "grid_dim = math.ceil( data_dim / THREADS ) \n",
    "\n",
    "block_dims = ( THREADS, 1, 1 )\n",
    "grid_dims = ( grid_dim, 1, 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate memory and copy data from host to device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a_gpu = cuda.mem_alloc( a.nbytes )\n",
    "cuda.memcpy_htod( a_gpu, a )\n",
    "\n",
    "b = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu = cuda.mem_alloc( b.nbytes )\n",
    "\n",
    "b_noshared = np.float32( np.empty( [ 1, data_dim-1] ) )\n",
    "b_gpu_noshared = cuda.mem_alloc( b_noshared.nbytes )\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the CUDA kernel and copy the result back to host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying constant THREADS for shared memory kernel\n",
    "kernel = kernel_code % {\n",
    "        'THREADS': THREADS\n",
    "        }\n",
    "\n",
    "# Compile the shared memory kernel code\n",
    "mod = SourceModule( kernel )\n",
    "\n",
    "diff_gpu = mod.get_function( \"vec_diff_sharedmem\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu( a_gpu, b_gpu, np.int32( data_dim ), \n",
    "        block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b, b_gpu )\n",
    "\n",
    "###### Doing the same as above for non-shared memory version\n",
    "diff_gpu_noshared = mod.get_function( \"vec_diff\" )\n",
    "\n",
    "# Measure gpu execution time\n",
    "gpu_start = timeit.default_timer()   \n",
    " \n",
    "diff_gpu_noshared( a_gpu, b_gpu_noshared, np.int32( data_dim ), \n",
    "                block = block_dims, grid = grid_dims )\n",
    "\n",
    "gpu_time_noshared = ( timeit.default_timer() - gpu_start ) * 1000\n",
    "\n",
    "cuda.memcpy_dtoh( b_noshared, b_gpu_noshared )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors are the same. \n",
      "CPU: 0.8452000038232654 ms\n",
      "GPU, no shared memory: 0.09529999806545675 ms\n",
      "GPU, shared memory: 0.14989999181125313 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure cpu time\n",
    "cpu_start = timeit.default_timer()  \n",
    " \n",
    "b_cpu = np.diff( a )\n",
    "\n",
    "cpu_time = ( timeit.default_timer() - cpu_start ) * 1000\n",
    "\n",
    "if ( b_cpu == b ).all() and (b_cpu == b_noshared ).all():\n",
    "    print( \"The vectors are the same. \\nCPU: \" + str( cpu_time )+ \" ms\\nGPU, no shared memory: \" \n",
    "            + str( gpu_time_noshared ) + \" ms\\nGPU, shared memory: \" + str( gpu_time ) + \" ms\")   \n",
    "else:\n",
    "    print( \"The vector are not the same. Something is wrong.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory version is slower. Bug in code, or is this kind of problem just generally more inefficient with shared memory? On the other hand, if running no shared memory first the shared memory version last, shared is (sometimes) faster. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Implement the three reduction models presented in the lectures. Time their performance against different vector sizes. Execution times can vary between executions, so run them for example 100 times and take the average time.\n",
    "\n",
    "Add also CPU performance with numpy sum() - function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = np.array( [ 2**10, 2**13, 2**16, 2**20, 2**22 ] )\n",
    "\n",
    "exec_times_gpu = np.zeros( ( 4, np.size( data_sizes ) ) )\n",
    "exec_times_cpu = np.empty_like( exec_times_gpu ).astype( np.float64 )\n",
    "\n",
    "threads_per_block = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = ( \"\"\"\n",
    "               \n",
    "  __global__ void interleaved( double* a, int data_length ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    for ( int ii = 1; ii < data_length; ii *= 2) {\n",
    "      \n",
    "      if ( index % ( 2*ii ) == 0 ) {\n",
    "        \n",
    "        a[ index ] += a[ index + ii ];\n",
    "        \n",
    "      }\n",
    "      \n",
    "    }\n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void sequential( double* a, int data_size ) {\n",
    "  \n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    for ( int ii = data_size/2; ii > 0; ii >>= 2) {\n",
    "      \n",
    "      if ( index < ii ) {\n",
    "        \n",
    "        a[ index ] += a[ index + ii ];\n",
    "        \n",
    "      }\n",
    "      \n",
    "    }\n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void interleaved_shared( float* a, int stride ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "  __global__ void sequential_shared( float* a, int stride ) {\n",
    "  \n",
    "    \n",
    "     \n",
    "  }\n",
    "  \n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.729268505485205\n",
      "26.729268505485212\n",
      "64.0932039416234\n",
      "52.124027359296306\n",
      "154.86710260436007\n",
      "140.05030336200062\n",
      "-253.00219978239016\n",
      "619.3809498644767\n",
      "165.83210243644857\n",
      "1682.8800849557172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_kernel = SourceModule( kernel_code ).get_function( 'interleaved' )\n",
    "\n",
    "# Looping through data sizes, doig the calculations and saving results.\n",
    "ii = 0\n",
    "jj = 0\n",
    "for size in data_sizes:    \n",
    "    \n",
    "    result = np.zeros( (1, size) ).astype( np.float64 )\n",
    "    \n",
    "    # Specifying thread and block dimensions for kernel call\n",
    "    block_dims = ( threads_per_block, 1, 1 )\n",
    "    grid_dims = ( math.ceil( size / threads_per_block), 1, 1 )\n",
    "    \n",
    "    # Create data and allocate memory\n",
    "    a = np.random.randn( 1, size ).astype( np.float64 )\n",
    "    a_gpu = cuda.mem_alloc( a.nbytes )\n",
    "    \n",
    "    cuda.memcpy_htod( a_gpu, a )\n",
    "    \n",
    "    # Calling kernel and copying result back to host\n",
    "    sum_kernel( a_gpu, np.int32( size ), block = block_dims, grid = grid_dims )\n",
    "    \n",
    "    cuda.memcpy_dtoh( result, a_gpu )\n",
    "    \n",
    "    print(result[0][0])\n",
    "    print( np.sum(a))\n",
    "    \n",
    "    exec_times_gpu[ii][jj] = result[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrng with vec sizes greater than 2 ** 10 ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
