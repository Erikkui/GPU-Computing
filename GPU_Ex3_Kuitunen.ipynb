{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM40A1401 GPU Computing\n",
    "\n",
    "## Erik Kuitunen\n",
    "\n",
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Both NumPy and CuPy offer the linear algebra functions  linalg.norm() to get the norm and linalg.qr() to get the QR decomposition of matrices.\n",
    "\n",
    "Time their performance against different sizes of square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating data and perfroming the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_elems = [ 10, 100, 250, 500, 1000, 1500 ] \n",
    "\n",
    "times_cpu = np.zeros( ( np.size( N_elems ), 2 ) )\n",
    "times_gpu = np.zeros( ( np.size( N_elems ), 2 ) )\n",
    "\n",
    "for jj in range( 100 ):\n",
    "    ii = 0\n",
    "    for elem in N_elems:\n",
    "        \n",
    "        # CPU measurements\n",
    "        a_cpu = np.random.randn( elem, elem )\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        res = np.linalg.norm( a_cpu )\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        times_cpu[ ii, 0 ] = ( end_time - start_time )*1000\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        res = np.linalg.qr( a_cpu )\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        times_cpu[ ii, 1 ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        \n",
    "        # GPU measurements\n",
    "        a_gpu = cp.random.randn( elem, elem )\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        res = cp.linalg.norm( a_gpu )\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        times_gpu[ ii, 0 ] = ( end_time - start_time )*1000\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        res = cp.linalg.qr( a_gpu )\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        times_gpu[ ii, 1 ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        ii += 1\n",
    "    \n",
    "times_cpu = times_cpu / 100\n",
    "times_gpu = times_gpu / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( N_elems, times_cpu[ :, 0 ], 'b', label = \"CPU\" )\n",
    "plt.plot( N_elems, times_gpu[ :, 0 ], 'k', label = \"GPU\")\n",
    "\n",
    "plt.title( \"CPU vs GPU, norm\")\n",
    "plt.xlabel( 'Amount of data in one dimension' )\n",
    "plt.ylabel( 'Time [ms]' )\n",
    "plt.legend( loc = \"upper left\" )\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot( N_elems, times_cpu[ :, 1 ], 'b', label = \"CPU\")\n",
    "plt.plot( N_elems, times_gpu[ :, 1 ], 'k',label = \"GPU\")\n",
    "\n",
    "plt.title( \"CPU vs GPU, QR decomposition\")\n",
    "plt.xlabel( 'Amount of data in one dimension')\n",
    "plt.ylabel( 'Time [ms]' )\n",
    "plt.legend( loc = \"upper left\" )\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Implement vector addition using NumPy, CuPy and an example kernel used in previous exercises. Time their performance against different vector sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "N_elems = [ 100, 10**3, 10**4, 10**5, 10**6, 5*10**6, 10**7 ] \n",
    "\n",
    "times_numpy = np.zeros( ( np.size( N_elems ), 1 ) )\n",
    "times_cupy = np.zeros( ( np.size( N_elems ), 1 ) )\n",
    "times_kernel = np.zeros( ( np.size( N_elems ), 1 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same N vector as in previous task will be used. \n",
    "\n",
    "Functions and kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NumPy vector addition\n",
    "def vecAdd_numpy( a, b ):\n",
    "    \n",
    "    vector_length = np.size(a)\n",
    "    #c = np.zeros( ( vector_length, 1) )\n",
    "\n",
    "    c = a + b\n",
    "    #for ii in range( vector_length ):\n",
    "    #    c[ ii ] = a[ii] + b[ii]\n",
    "    return c\n",
    "\n",
    "# CuPy vector addition\n",
    "def vecAdd_cupy( a, b ):\n",
    "    \n",
    "    #vector_length = cp.size(a)\n",
    "    #c = cp.zeros( ( vector_length, 1) )\n",
    "\n",
    "    c = cp.add( a, b )\n",
    "    \n",
    "# PyCUDA vector addition\n",
    "modd = SourceModule(\"\"\"\n",
    "  __global__ void vector_addition( double* a, double* b, double* c, int n_elem) {\n",
    "\n",
    "    for ( int i = threadIdx.x + blockIdx.x * blockDim.x; \n",
    "          i < n_elem; \n",
    "          i += gridDim.x * blockDim.x ) {\n",
    "            \n",
    "        c[i] = a[i] + b[i];\n",
    "    \n",
    "    }\n",
    "  }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elems in N_elems: \n",
    "        \n",
    "        a = np.random.randn( elems, 1 ).astype( float )\n",
    "        b = np.random.randn( elems, 1 ).astype( float )\n",
    "        \n",
    "        # Calling NumPy function and saving result\n",
    "        start_time = time.perf_counter()\n",
    "        res = vecAdd_numpy( a, b )\n",
    "        end_time = time.perf_counter()\n",
    "        times_numpy[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_numpy = times_numpy/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elems in N_elems:\n",
    "        \n",
    "        a = cp.random.randn( elems, 1 ).astype( float )\n",
    "        b = cp.random.randn( elems, 1 ).astype( float )\n",
    "        \n",
    "        # Calling CuPy function and saving result\n",
    "        start_time = time.perf_counter()\n",
    "        res = vecAdd_cupy( a, b )\n",
    "        end_time = time.perf_counter()\n",
    "        times_cupy[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_cupy = times_cupy/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCUDA calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elems in N_elems:\n",
    "        \n",
    "        a = np.random.randn( elems, 1 ).astype( float )\n",
    "        b = np.random.randn( elems, 1 ).astype( float )\n",
    "        \n",
    "        block_dims = ( 1024, 1, 1 )\n",
    "        grid_dims = ( int( np.ceil( elems / 1024 ) ), 1, 1 )\n",
    "        \n",
    "        a_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n",
    "        cuda.memcpy_htod( a_gpu, a )\n",
    "\n",
    "        b_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n",
    "        cuda.memcpy_htod( b_gpu, b )\n",
    "\n",
    "        c = np.empty_like(a)\n",
    "        c_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n",
    "        \n",
    "        kernel = modd.get_function( \"vector_addition\" )\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        kernel( a_gpu, b_gpu, c_gpu, np.int32( elems ), block = block_dims, grid = grid_dims )\n",
    "        end_time = time.perf_counter()\n",
    "        times_kernel[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        cuda.memcpy_dtoh( c, c_gpu )\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_kernel = times_kernel/100\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale( \"log\" )\n",
    "plt.yscale( \"log\" )\n",
    "plt.plot( N_elems, times_cupy, 'k', label = \"CuPy\")\n",
    "plt.plot( N_elems, times_kernel, 'r', label = \"PyCUDA\")\n",
    "plt.plot( N_elems[ 0:np.size( times_numpy ) ], times_numpy, 'b', label = \"NumPy\" )\n",
    "plt.xlabel( 'Amount of data in one dimension' )\n",
    "plt.ylabel( 'Time [ms]' )\n",
    "plt.legend( loc = \"upper left\" )\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "Implement matrix multiplication using NumPy, CuPy and an example kernel used in previous exercises. Time their performance against different matrix sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing data and defining functions and kernel to be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_elems = [ 10, 100, 250, 500, 1000, 1500, 2000 ] \n",
    "\n",
    "times_numpy = np.zeros( ( np.size( N_elems ), 1 ) )\n",
    "times_cupy = np.zeros( ( np.size( N_elems ), 1 ) )\n",
    "times_kernel = np.zeros( ( np.size( N_elems ), 1 ) )\n",
    "\n",
    "def matmul_numpy( A, B ):\n",
    "    \n",
    "    C = np.matmul( A, B )\n",
    "    \n",
    "    return C\n",
    "\n",
    "def matmul_cupy( A, B ):\n",
    "    \n",
    "    C = cp.matmul( A, B )\n",
    "    \n",
    "    return C\n",
    "\n",
    "kernel_code_template = \"\"\"\n",
    "  __global__ void matmul_sharedmem( const float* A, const float* B, float* C, int M, int N, int K, int n_tiles ) {\n",
    "    \n",
    "    float C_elem = 0;\n",
    "    \n",
    "    // Block indices, each block computes submatrix of C, C_sub\n",
    "    int block_row = blockIdx.y;\n",
    "    int block_col = blockIdx.x;\n",
    "    \n",
    "    // Thread indices. Each thread computes an element of C_sub\n",
    "    int thread_row = threadIdx.y;\n",
    "    int thread_col = threadIdx.x;\n",
    "\n",
    "    // Looping through relevant submatrices to compute C_sub\n",
    "    for (int ii = 0; ii < n_tiles; ++ii ) { \n",
    "    \n",
    "      // Loading submatrices from global memory\n",
    "      int linear_ind_A = N * thread_row + thread_col + ii * %(TILE_WIDTH)s + block_row * %(TILE_WIDTH)s * N;\n",
    "      int linear_ind_B = K * thread_row + thread_col + ii * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s;\n",
    "      \n",
    "      // Shared memory for the submatrices of A and B\n",
    "      __shared__ float A_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n",
    "      __shared__ float B_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n",
    "      \n",
    "      // TODO check for threads outside matrix bounds\n",
    "      \n",
    "      A_sub[ thread_row ][ thread_col ] = A[ linear_ind_A ];\n",
    "      B_sub[ thread_row ][ thread_col ] = B[ linear_ind_B ];\n",
    "      \n",
    "      __syncthreads();\n",
    "      \n",
    "      // Doing the actual multiplication of the submatrices\n",
    "      for (int kk = 0; kk < %(TILE_WIDTH)s; ++kk) {\n",
    "        \n",
    "        float A_sub_elem = A_sub[ thread_row ][ kk ];\n",
    "        float B_sub_elem = B_sub[ kk ][ thread_col ];\n",
    "        \n",
    "        C_elem += A_sub_elem * B_sub_elem;\n",
    "        \n",
    "      }\n",
    "      \n",
    "      __syncthreads();\n",
    "    \n",
    "    }\n",
    "      \n",
    "    // Saving the C_elem to matrix C\n",
    "    int linear_ind_C = block_row * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s + thread_row * K + thread_col;\n",
    "    C[ linear_ind_C ] = C_elem;\n",
    "    \n",
    "  } \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elems in N_elems:\n",
    "        \n",
    "        A = np.random.randn( elems, elems ).astype( float )\n",
    "        B = np.random.randn( elems, elems ).astype( float )\n",
    "        \n",
    "        # Calling NumPy function and saving result\n",
    "        start_time = time.perf_counter()\n",
    "        res = matmul_numpy( A, B )\n",
    "        end_time = time.perf_counter()\n",
    "        times_numpy[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_numpy = times_numpy/100\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elems in N_elems:\n",
    "        \n",
    "        A = cp.random.randn( elems, elems ).astype( float )\n",
    "        B = cp.random.randn( elems, elems ).astype( float )\n",
    "        \n",
    "        # Calling CuPy function and saving result\n",
    "        start_time = time.perf_counter()\n",
    "        res = matmul_cupy( A, B )\n",
    "        end_time = time.perf_counter()\n",
    "        times_cupy[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_cupy = times_cupy/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing PyCUDA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_WIDTH = 16\n",
    "\n",
    "kernel_code = kernel_code_template % {\n",
    "        'TILE_WIDTH': TILE_WIDTH\n",
    "        }\n",
    "mod = SourceModule( kernel_code )\n",
    "matriximul = mod.get_function( \"matmul_sharedmem\" )\n",
    "\n",
    "for ii in range( 100 ):\n",
    "    jj = 0\n",
    "    for elem in N_elems:\n",
    "        \n",
    "        # Creating data, initializing thread and block sizes\n",
    "        A = np.float32( np.random.rand( elem, elem ) )\n",
    "        B = np.float32( np.random.rand( elem, elem ) )\n",
    "\n",
    "        gridDim = int( elem/TILE_WIDTH )\n",
    "        block_dims = ( TILE_WIDTH, TILE_WIDTH, 1 )\n",
    "        grid_dims = ( gridDim, gridDim, 1 )\n",
    "        \n",
    "        # Allocating memory and copying data from host to device\n",
    "        A_gpu = cuda.mem_alloc( A.nbytes )\n",
    "        cuda.memcpy_htod( A_gpu, A )\n",
    "\n",
    "        B_gpu = cuda.mem_alloc( B.nbytes )\n",
    "        cuda.memcpy_htod( B_gpu, B )\n",
    "\n",
    "        C = np.empty( [ A.shape[0], B.shape[1] ], dtype = np.float32 )\n",
    "        C_gpu = cuda.mem_alloc( C.nbytes )\n",
    "        \n",
    "        # Call the kernel\n",
    "        dim = np.int32( elem )\n",
    "        tiles = np.int32( np.ceil( elem/TILE_WIDTH))\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        matriximul( A_gpu, B_gpu, C_gpu, dim, dim, dim, tiles,\n",
    "                   block = block_dims, grid = grid_dims )\n",
    "        end_time = time.perf_counter()\n",
    "        times_kernel[ jj ] += ( end_time - start_time )*1000\n",
    "        \n",
    "        cuda.memcpy_dtoh( C, C_gpu )\n",
    "        \n",
    "        jj += 1\n",
    "        \n",
    "times_kernel = times_kernel/100    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
