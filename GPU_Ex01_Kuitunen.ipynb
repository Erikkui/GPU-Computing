{"cells":[{"cell_type":"markdown","metadata":{},"source":["# BM40A1401 GPU Computing\n","\n","## Erik Kuitunen\n","\n","### Exercise 1"]},{"cell_type":"markdown","metadata":{"id":"OqJ_QFMHgH-C"},"source":["Import needed libraries."]},{"cell_type":"code","execution_count":1233,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153431,"status":"ok","timestamp":1704708100798,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"vHANHt1Pfxvy","outputId":"7bfb4420-5df9-4fe0-9b0e-e966b42e9660"},"outputs":[],"source":["#!pip install pycuda\n","import pycuda.autoinit\n","import pycuda.driver as cuda\n","from pycuda.compiler import SourceModule\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 1\n","Implement a kernel which takes two vectors A and B and adds them together to form a vector C."]},{"cell_type":"markdown","metadata":{"id":"ws4azXoLgyDV"},"source":["Derfining the kernel."]},{"cell_type":"code","execution_count":1234,"metadata":{"executionInfo":{"elapsed":589,"status":"ok","timestamp":1704708787763,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"MXmfbzNGg2sN"},"outputs":[],"source":["modd = SourceModule(\"\"\"\n","  __global__ void vector_addition( double* a, double* b, double* c, int n_elem) {\n","\n","    for ( int i = threadIdx.x + blockIdx.x * blockDim.x; \n","          i < n_elem; \n","          i += gridDim.x * blockDim.x ) {\n","            \n","      c[i] = a[i] + b[i];\n","    \n","    }\n","  }\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["Create data vectors and initalize thread and block sizes"]},{"cell_type":"code","execution_count":1235,"metadata":{},"outputs":[],"source":["N = 10 ** 5\n","a = np.random.randn(N).astype( float )\n","b = np.random.randn(N).astype( float )\n","\n","block_dims = ( 1024, 1, 1 )\n","grid_dims = ( 64, 1, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory to GPU and copy data to device"]},{"cell_type":"code","execution_count":1236,"metadata":{},"outputs":[],"source":["a_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n","cuda.memcpy_htod( a_gpu, a )\n","\n","b_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n","cuda.memcpy_htod( b_gpu, b )\n","\n","c = np.empty_like(a)\n","c_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )"]},{"cell_type":"markdown","metadata":{"id":"J24uGIs1hmyU"},"source":["Calling the CUDA kernel."]},{"cell_type":"code","execution_count":1237,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1704708830776,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"qKyyajYLhrT-","outputId":"c3d7fad6-cd64-4fe1-f8f5-03d907ffcd6f"},"outputs":[],"source":["kernel = modd.get_function( \"vector_addition\" )\n","\n","kernel( a_gpu, b_gpu, c_gpu, np.int32(N), block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":1238,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The vectors are the same.\n"]}],"source":["cuda.memcpy_dtoh( c, c_gpu )\n","\n","c_cpu = a + b\n","\n","if ( c_cpu == c ).all():\n","    print( \"The vectors are the same.\" )   \n","else:\n","    print( \"The vector are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 2\n","\n","Implement a kernel which multiplies two matrices together."]},{"cell_type":"markdown","metadata":{},"source":["Defining the kernel"]},{"cell_type":"code","execution_count":1239,"metadata":{},"outputs":[],"source":["modd = SourceModule(\"\"\"\n","  __global__ void matrix_multiplication( const float* A, const float* B, float* C, int M, int N, int K) {\n","    \n","    int row = threadIdx.y; \n","    int col = threadIdx.x;\n","    float C_elem = 0;\n","    \n","    if ( row > K-1 || col > K-1 ) {\n","      return;\n","    }\n","    \n","    for ( int ii = 0; ii < N; ++ii ) {\n","        \n","      float A_elem = A[ row * N + ii ];\n","      float B_elem = B[ col + K * ii ];\n","\n","      C_elem += A_elem * B_elem;\n","    \n","    }\n","    \n","    C[ col + row * K ] = C_elem;\n","    \n","  } \n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["Create data matrices and initalize thread and block sizes"]},{"cell_type":"code","execution_count":1240,"metadata":{},"outputs":[],"source":["BLOCK_SIZE = 16\n","\n","M = 14   # Rows of A and C\n","N = 15   # Columns of A; rows of B\n","K = 16   # Columns of B and C\n","\n","A = np.float32( np.random.rand( M, N ) )\n","B = np.float32( np.random.rand( N, K ) )\n","\n","block_dims = ( BLOCK_SIZE, BLOCK_SIZE, 1 )\n","grid_dims = ( 1, 1, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory and copy data from host to device "]},{"cell_type":"code","execution_count":1241,"metadata":{},"outputs":[],"source":["A_gpu = cuda.mem_alloc( A.nbytes )\n","cuda.memcpy_htod( A_gpu, A )\n","\n","B_gpu = cuda.mem_alloc( B.nbytes )\n","cuda.memcpy_htod( B_gpu, B )\n","\n","C = np.empty( [ A.shape[0], B.shape[1] ], dtype = np.float32 )\n","C_gpu = cuda.mem_alloc( C.nbytes )\n"]},{"cell_type":"markdown","metadata":{},"source":["Calling the CUDA kernel."]},{"cell_type":"code","execution_count":1242,"metadata":{},"outputs":[],"source":["kernel = modd.get_function( \"matrix_multiplication\" )\n","\n","kernel( A_gpu, B_gpu, C_gpu, np.int32(M), np.int32(N), np.int32(K),\n","        block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":1243,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The result matrices are (nearly) the same.\n"]}],"source":["cuda.memcpy_dtoh( C, C_gpu )\n","\n","C_cpu = np.dot( A, B )\n","\n","C_diff = abs( C_cpu - C )\n","\n","if ( C_diff.all() < 10 ** -6 ):\n","    print( \"The result matrices are (nearly) the same.\" )   \n","else:\n","    print( \"The matrices are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["Differences in the order of $ 10^7 $ can be found from the results. Why?\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 3\n","\n","Extend the kernel from task 2 to use shared memory."]},{"cell_type":"code","execution_count":1244,"metadata":{},"outputs":[],"source":["kernel_code_template = \"\"\"\n","  __global__ void matmul_sharedmem( const float* A, const float* B, float* C, int M, int N, int K, int n_tiles ) {\n","    \n","    float C_elem = 0;\n","    \n","    // Block indices, each block computes submatrix of C, C_sub\n","    int block_row = blockIdx.y;\n","    int block_col = blockIdx.x;\n","    \n","    // Thread indices. Each thread computes an element of C_sub\n","    int thread_row = threadIdx.y;\n","    int thread_col = threadIdx.x;\n","\n","    // Looping through relevant submatrices to compute C_sub\n","    for (int ii = 0; ii < n_tiles; ++ii ) { \n","    \n","      // Loading submatrices from global memory\n","      int linear_ind_A = N * thread_row + thread_col + ii * %(TILE_WIDTH)s + block_row * %(TILE_WIDTH)s * N;\n","      int linear_ind_B = K * thread_row + thread_col + ii * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s;\n","      \n","      // Shared memory for the submatrices of A and B\n","      __shared__ float A_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n","      __shared__ float B_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n","      \n","      // TODO check for threads outside matrix bounds\n","      \n","      A_sub[ thread_row ][ thread_col ] = A[ linear_ind_A ];\n","      B_sub[ thread_row ][ thread_col ] = B[ linear_ind_B ];\n","      \n","      __syncthreads();\n","      \n","      // Doin the actual multiplication of the submatrices\n","      for (int kk = 0; kk < %(TILE_WIDTH)s; ++kk) {\n","        \n","        float A_sub_elem = A_sub[ thread_row ][ kk ];\n","        float B_sub_elem = B_sub[ kk ][ thread_col ];\n","        \n","        C_elem += A_sub_elem * B_sub_elem;\n","        \n","      }\n","      \n","      __syncthreads();\n","    \n","    }\n","      \n","    // Saving the C_elem to matrix C\n","    int linear_ind_C = block_row * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s + thread_row * K + thread_col;\n","    C[ linear_ind_C ] = C_elem;\n","    \n","  } \n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["Create data vectors and initalize thread and block sizes"]},{"cell_type":"code","execution_count":1245,"metadata":{},"outputs":[],"source":["TILE_WIDTH = 4\n","\n","M = 8\n","N = M\n","K = M\n","\n","testarr1 = np.array( [1, 1, 1, 1], dtype = np.float32 )\n","testarr2 = np.array( [1, 2, 3, 4], dtype = np.float32 )\n","A = np.array( [ 1*testarr1, 2*testarr1, 3*testarr1, 4*testarr1 ] )\n","B = np.array( [ testarr2, testarr2, testarr2, testarr2 ] )\n","\n","A = np.float32( np.random.rand( M, N ) )\n","B = np.float32( np.random.rand( N, K ) )\n","\n","gridDim = int( N/TILE_WIDTH )\n","block_dims = ( TILE_WIDTH, TILE_WIDTH, 1 )\n","grid_dims = ( gridDim, gridDim, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory and copy data from host to device "]},{"cell_type":"code","execution_count":1246,"metadata":{},"outputs":[],"source":["A_gpu = cuda.mem_alloc( A.nbytes )\n","cuda.memcpy_htod( A_gpu, A )\n","\n","B_gpu = cuda.mem_alloc( B.nbytes )\n","cuda.memcpy_htod( B_gpu, B )\n","\n","C = np.empty( [ A.shape[0], B.shape[1] ], dtype = np.float32 )\n","C_gpu = cuda.mem_alloc( C.nbytes )"]},{"cell_type":"markdown","metadata":{},"source":["Specify constant TILE_WIDTH for the kernel. Compile and call the kernel."]},{"cell_type":"code","execution_count":1247,"metadata":{},"outputs":[],"source":["kernel_code = kernel_code_template % {\n","        'TILE_WIDTH': TILE_WIDTH\n","        }\n","\n","# Compile the kernel code\n","mod = SourceModule( kernel_code )\n","\n","matrixmul = mod.get_function( \"matmul_sharedmem\" )\n","\n","matrixmul( A_gpu, B_gpu, C_gpu, np.int32(M), np.int32(N), np.int32(K), np.int32(2),\n","          block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":1248,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU: \n","[[1.0189475 2.4833114 1.7240088 2.2825298 1.4542924 2.5112624 2.4092796\n","  1.083016 ]\n"," [1.5417008 2.323924  1.7720449 2.546152  1.8667597 2.592002  2.2686083\n","  1.3252709]\n"," [1.811689  2.2873244 2.4921846 2.9266448 2.3186917 3.2190785 2.462441\n","  1.6410722]\n"," [1.909859  3.026258  2.5802743 3.3437405 2.1028512 3.2470243 2.4857454\n","  1.9692321]\n"," [0.875985  1.1321583 1.4496667 1.5337021 1.1134462 1.7278705 1.2545614\n","  0.9610581]\n"," [1.8450305 2.7292013 2.1018882 3.032506  2.3212109 3.114973  2.6378472\n","  1.7493572]\n"," [1.7519978 2.838317  1.9783143 2.8842294 2.4369698 3.2436905 3.1015098\n","  1.5551019]\n"," [1.5662793 2.6651707 1.926076  2.6647525 1.8791857 2.734553  2.364956\n","  1.6191969]]\n","\n","GPU: \n","[[1.0189476 2.4833114 1.7240087 2.2825298 1.4542924 2.5112624 2.4092796\n","  1.0830162]\n"," [1.5417008 2.323924  1.772045  2.5461524 1.8667597 2.5920024 2.268608\n","  1.325271 ]\n"," [1.811689  2.2873242 2.4921846 2.9266448 2.318692  3.2190785 2.462441\n","  1.641072 ]\n"," [1.9098588 3.026258  2.5802746 3.3437402 2.1028512 3.2470245 2.4857454\n","  1.9692321]\n"," [0.875985  1.1321584 1.4496669 1.5337021 1.1134464 1.7278705 1.2545614\n","  0.9610581]\n"," [1.8450304 2.729201  2.101888  3.0325062 2.3212106 3.114973  2.6378472\n","  1.7493571]\n"," [1.7519978 2.838317  1.9783145 2.8842294 2.4369695 3.2436907 3.1015098\n","  1.555102 ]\n"," [1.5662792 2.6651707 1.926076  2.6647525 1.8791856 2.734553  2.3649557\n","  1.6191968]]\n","\n","Difference: \n","[[1.1920929e-07 0.0000000e+00 1.1920929e-07 0.0000000e+00 0.0000000e+00\n","  0.0000000e+00 0.0000000e+00 1.1920929e-07]\n"," [0.0000000e+00 0.0000000e+00 1.1920929e-07 2.3841858e-07 0.0000000e+00\n","  4.7683716e-07 2.3841858e-07 1.1920929e-07]\n"," [0.0000000e+00 2.3841858e-07 0.0000000e+00 0.0000000e+00 2.3841858e-07\n","  0.0000000e+00 0.0000000e+00 1.1920929e-07]\n"," [1.1920929e-07 0.0000000e+00 2.3841858e-07 2.3841858e-07 0.0000000e+00\n","  2.3841858e-07 0.0000000e+00 0.0000000e+00]\n"," [0.0000000e+00 1.1920929e-07 1.1920929e-07 0.0000000e+00 1.1920929e-07\n","  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n"," [1.1920929e-07 2.3841858e-07 2.3841858e-07 2.3841858e-07 2.3841858e-07\n","  0.0000000e+00 0.0000000e+00 1.1920929e-07]\n"," [0.0000000e+00 0.0000000e+00 2.3841858e-07 0.0000000e+00 2.3841858e-07\n","  2.3841858e-07 0.0000000e+00 1.1920929e-07]\n"," [1.1920929e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1920929e-07\n","  0.0000000e+00 2.3841858e-07 1.1920929e-07]]\n","The result matrices are (nearly) the same.\n"]}],"source":["cuda.memcpy_dtoh( C, C_gpu )\n","\n","C_cpu = np.dot( A, B )\n","\n","print(\"CPU: \")\n","print( C_cpu)\n","\n","print( \"\\nGPU: \")\n","print( C )\n","\n","C_diff = abs( C_cpu - C )\n","print( \"\\nDifference: \")\n","print( C_diff )\n","\n","if ( C_diff.all() < 10 ** -6 ):\n","    print( \"The result matrices are (nearly) the same.\" )   \n","else:\n","    print( \"The matrices are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["Once again differences are appearing. Seems to be related to the ratio of the size of matrix and tile width?"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPpM2cfgi1eNvtNcnxiT2sO","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
