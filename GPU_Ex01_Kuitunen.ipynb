{"cells":[{"cell_type":"markdown","metadata":{},"source":["# BM40A1401 GPU Computing\n","\n","## Erik Kuitunen\n","\n","### Exercise 1"]},{"cell_type":"markdown","metadata":{"id":"OqJ_QFMHgH-C"},"source":["Import needed libraries."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153431,"status":"ok","timestamp":1704708100798,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"vHANHt1Pfxvy","outputId":"7bfb4420-5df9-4fe0-9b0e-e966b42e9660"},"outputs":[],"source":["#!pip install pycuda\n","import pycuda.autoinit\n","import pycuda.driver as cuda\n","from pycuda.compiler import SourceModule\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 1\n","Implement a kernel which takes two vectors A and B and adds them together to form a vector C."]},{"cell_type":"markdown","metadata":{"id":"ws4azXoLgyDV"},"source":["Derfining the kernel."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":589,"status":"ok","timestamp":1704708787763,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"MXmfbzNGg2sN"},"outputs":[],"source":["modd = SourceModule(\"\"\"\n","  __global__ void vector_addition( double* a, double* b, double* c, int n_elem) {\n","\n","    for ( int i = threadIdx.x + blockIdx.x * blockDim.x; \n","          i < n_elem; \n","          i += gridDim.x * blockDim.x ) {\n","            \n","      c[i] = a[i] + b[i];\n","    \n","    }\n","  }\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["Create data vectors and initalize thread and block sizes"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["N = 10 ** 5\n","a = np.random.randn(N).astype( float )\n","b = np.random.randn(N).astype( float )\n","\n","block_dims = ( 1024, 1, 1 )\n","grid_dims = ( 64, 1, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory to GPU and copy data to device"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["a_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n","cuda.memcpy_htod( a_gpu, a )\n","\n","b_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )\n","cuda.memcpy_htod( b_gpu, b )\n","\n","c = np.empty_like(a)\n","c_gpu = cuda.mem_alloc( a.size * a.dtype.itemsize )"]},{"cell_type":"markdown","metadata":{"id":"J24uGIs1hmyU"},"source":["Calling the CUDA kernel."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1704708830776,"user":{"displayName":"Henri Petrow","userId":"15597565801323973644"},"user_tz":-120},"id":"qKyyajYLhrT-","outputId":"c3d7fad6-cd64-4fe1-f8f5-03d907ffcd6f"},"outputs":[],"source":["kernel = modd.get_function( \"vector_addition\" )\n","\n","kernel( a_gpu, b_gpu, c_gpu, np.int32(N), block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The vectors are the same.\n"]}],"source":["cuda.memcpy_dtoh( c, c_gpu )\n","\n","c_cpu = a + b\n","\n","if ( c_cpu == c ).all():\n","    print( \"The vectors are the same.\" )   \n","else:\n","    print( \"The vector are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 2\n","\n","Implement a kernel which multiplies two matrices together."]},{"cell_type":"markdown","metadata":{},"source":["Defining the kernel"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["modd = SourceModule(\"\"\"\n","  __global__ void matrix_multiplication( const float* A, const float* B, float* C, int M, int N, int K) {\n","    \n","    int row = threadIdx.y; \n","    int col = threadIdx.x;\n","    float C_elem = 0;\n","    \n","    if ( row > K-1 || col > K-1 ) {\n","      return;\n","    }\n","    \n","    for ( int ii = 0; ii < N; ++ii ) {\n","        \n","      float A_elem = A[ row * N + ii ];\n","      float B_elem = B[ col + K * ii ];\n","\n","      C_elem += A_elem * B_elem;\n","    \n","    }\n","    \n","    C[ col + row * K ] = C_elem;\n","    \n","  } \n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["Create data matrices and initalize thread and block sizes"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["BLOCK_SIZE = 16\n","\n","M = 14   # Rows of A and C\n","N = 15   # Columns of A; rows of B\n","K = 16   # Columns of B and C\n","\n","A = np.float32( np.random.rand( M, N ) )\n","B = np.float32( np.random.rand( N, K ) )\n","\n","block_dims = ( BLOCK_SIZE, BLOCK_SIZE, 1 )\n","grid_dims = ( 1, 1, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory and copy data from host to device "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["A_gpu = cuda.mem_alloc( A.nbytes )\n","cuda.memcpy_htod( A_gpu, A )\n","\n","B_gpu = cuda.mem_alloc( B.nbytes )\n","cuda.memcpy_htod( B_gpu, B )\n","\n","C = np.empty( [ A.shape[0], B.shape[1] ], dtype = np.float32 )\n","C_gpu = cuda.mem_alloc( C.nbytes )\n"]},{"cell_type":"markdown","metadata":{},"source":["Calling the CUDA kernel."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["kernel = modd.get_function( \"matrix_multiplication\" )\n","\n","kernel( A_gpu, B_gpu, C_gpu, np.int32(M), np.int32(N), np.int32(K),\n","        block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The result matrices are (nearly) the same.\n"]}],"source":["cuda.memcpy_dtoh( C, C_gpu )\n","\n","C_cpu = np.dot( A, B )\n","\n","C_diff = abs( C_cpu - C )\n","\n","if ( C_diff.all() < 10 ** -6 ):\n","    print( \"The result matrices are (nearly) the same.\" )   \n","else:\n","    print( \"The matrices are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["Differences in the order of $ 10^7 $ can be found from the results. Why?\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Task 3\n","\n","Extend the kernel from task 2 to use shared memory."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["kernel_code_template = \"\"\"\n","  __global__ void matmul_sharedmem( const float* A, const float* B, float* C, int M, int N, int K, int n_tiles ) {\n","    \n","    float C_elem = 0;\n","    \n","    // Block indices, each block computes submatrix of C, C_sub\n","    int block_row = blockIdx.y;\n","    int block_col = blockIdx.x;\n","    \n","    // Thread indices. Each thread computes an element of C_sub\n","    int thread_row = threadIdx.y;\n","    int thread_col = threadIdx.x;\n","\n","    // Looping through relevant submatrices to compute C_sub\n","    for (int ii = 0; ii < n_tiles; ++ii ) { \n","    \n","      // Loading submatrices from global memory\n","      int linear_ind_A = N * thread_row + thread_col + ii * %(TILE_WIDTH)s + block_row * %(TILE_WIDTH)s * N;\n","      int linear_ind_B = K * thread_row + thread_col + ii * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s;\n","      \n","      // Shared memory for the submatrices of A and B\n","      __shared__ float A_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n","      __shared__ float B_sub[ %(TILE_WIDTH)s ][ %(TILE_WIDTH)s ];\n","      \n","      // TODO check for threads outside matrix bounds\n","      \n","      A_sub[ thread_row ][ thread_col ] = A[ linear_ind_A ];\n","      B_sub[ thread_row ][ thread_col ] = B[ linear_ind_B ];\n","      \n","      __syncthreads();\n","      \n","      // Doin the actual multiplication of the submatrices\n","      for (int kk = 0; kk < %(TILE_WIDTH)s; ++kk) {\n","        \n","        float A_sub_elem = A_sub[ thread_row ][ kk ];\n","        float B_sub_elem = B_sub[ kk ][ thread_col ];\n","        \n","        C_elem += A_sub_elem * B_sub_elem;\n","        \n","      }\n","      \n","      __syncthreads();\n","    \n","    }\n","      \n","    // Saving the C_elem to matrix C\n","    int linear_ind_C = block_row * %(TILE_WIDTH)s * K + block_col * %(TILE_WIDTH)s + thread_row * K + thread_col;\n","    C[ linear_ind_C ] = C_elem;\n","    \n","  } \n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["Create data vectors and initalize thread and block sizes"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["TILE_WIDTH = 4\n","\n","M = 8\n","N = M\n","K = M\n","\n","testarr1 = np.array( [1, 1, 1, 1], dtype = np.float32 )\n","testarr2 = np.array( [1, 2, 3, 4], dtype = np.float32 )\n","A = np.array( [ 1*testarr1, 2*testarr1, 3*testarr1, 4*testarr1 ] )\n","B = np.array( [ testarr2, testarr2, testarr2, testarr2 ] )\n","\n","A = np.float32( np.random.rand( M, N ) )\n","B = np.float32( np.random.rand( N, K ) )\n","\n","gridDim = int( N/TILE_WIDTH )\n","block_dims = ( TILE_WIDTH, TILE_WIDTH, 1 )\n","grid_dims = ( gridDim, gridDim, 1 )"]},{"cell_type":"markdown","metadata":{},"source":["Allocate memory and copy data from host to device "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["A_gpu = cuda.mem_alloc( A.nbytes )\n","cuda.memcpy_htod( A_gpu, A )\n","\n","B_gpu = cuda.mem_alloc( B.nbytes )\n","cuda.memcpy_htod( B_gpu, B )\n","\n","C = np.empty( [ A.shape[0], B.shape[1] ], dtype = np.float32 )\n","C_gpu = cuda.mem_alloc( C.nbytes )"]},{"cell_type":"markdown","metadata":{},"source":["Specify constant TILE_WIDTH for the kernel. Compile and call the kernel."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["kernel_code = kernel_code_template % {\n","        'TILE_WIDTH': TILE_WIDTH\n","        }\n","\n","# Compile the kernel code\n","mod = SourceModule( kernel_code )\n","\n","matrixmul = mod.get_function( \"matmul_sharedmem\" )\n","\n","matrixmul( A_gpu, B_gpu, C_gpu, np.int32(M), np.int32(N), np.int32(K), np.int32(2),\n","          block = block_dims, grid = grid_dims )"]},{"cell_type":"markdown","metadata":{},"source":["Copying the results back to host and verifying the results"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU: \n","[[1.0978166 2.121004  1.2746941 1.5570467 1.772937  1.8167293 1.9460783\n","  1.266226 ]\n"," [1.764896  3.1360211 2.528822  2.6016932 2.6045198 2.1067991 3.147779\n","  2.2767608]\n"," [1.898014  2.4650378 2.2403572 1.967734  2.357735  2.2485762 2.4446454\n","  1.9174922]\n"," [1.0522815 1.4969549 1.2747726 1.0804694 1.3165609 1.213459  1.3916788\n","  1.2290797]\n"," [1.207422  2.4884174 1.5961732 2.0638947 2.2286196 1.8456924 2.8419242\n","  1.3923709]\n"," [2.1598887 2.4549973 2.5588758 2.2881522 2.5439339 1.9696869 2.142542\n","  1.7012703]\n"," [1.8302269 2.5439596 2.0368915 2.3302355 1.9327054 1.6085576 2.305274\n","  2.0962129]\n"," [1.7069124 2.3611412 1.994415  1.8300638 1.9015458 1.5489612 1.8538482\n","  1.9350185]]\n","\n","GPU: \n","[[1.0978166 2.121004  1.274694  1.5570465 1.772937  1.8167292 1.9460781\n","  1.266226 ]\n"," [1.7648959 3.136021  2.5288222 2.6016932 2.6045196 2.106799  3.1477787\n","  2.2767606]\n"," [1.8980138 2.4650378 2.2403572 1.967734  2.3577347 2.248576  2.4446456\n","  1.9174919]\n"," [1.0522815 1.496955  1.2747726 1.0804695 1.3165607 1.213459  1.3916788\n","  1.2290798]\n"," [1.2074221 2.4884174 1.596173  2.0638947 2.2286196 1.8456923 2.8419244\n","  1.3923709]\n"," [2.159889  2.4549973 2.558876  2.2881522 2.543934  1.9696869 2.142542\n","  1.7012705]\n"," [1.8302268 2.5439599 2.0368915 2.3302355 1.9327053 1.6085577 2.3052738\n","  2.096213 ]\n"," [1.7069124 2.361141  1.9944149 1.8300637 1.9015458 1.548961  1.8538482\n","  1.9350184]]\n","\n","Difference: \n","[[0.0000000e+00 0.0000000e+00 1.1920929e-07 1.1920929e-07 0.0000000e+00\n","  1.1920929e-07 2.3841858e-07 0.0000000e+00]\n"," [1.1920929e-07 2.3841858e-07 2.3841858e-07 0.0000000e+00 2.3841858e-07\n","  2.3841858e-07 2.3841858e-07 2.3841858e-07]\n"," [1.1920929e-07 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3841858e-07\n","  2.3841858e-07 2.3841858e-07 2.3841858e-07]\n"," [0.0000000e+00 1.1920929e-07 0.0000000e+00 1.1920929e-07 1.1920929e-07\n","  0.0000000e+00 0.0000000e+00 1.1920929e-07]\n"," [1.1920929e-07 0.0000000e+00 1.1920929e-07 0.0000000e+00 0.0000000e+00\n","  1.1920929e-07 2.3841858e-07 0.0000000e+00]\n"," [2.3841858e-07 0.0000000e+00 2.3841858e-07 0.0000000e+00 2.3841858e-07\n","  0.0000000e+00 0.0000000e+00 1.1920929e-07]\n"," [1.1920929e-07 2.3841858e-07 0.0000000e+00 0.0000000e+00 1.1920929e-07\n","  1.1920929e-07 2.3841858e-07 2.3841858e-07]\n"," [0.0000000e+00 2.3841858e-07 1.1920929e-07 1.1920929e-07 0.0000000e+00\n","  1.1920929e-07 0.0000000e+00 1.1920929e-07]]\n","The result matrices are (nearly) the same.\n"]}],"source":["cuda.memcpy_dtoh( C, C_gpu )\n","\n","C_cpu = np.dot( A, B )\n","\n","print(\"CPU: \")\n","print( C_cpu)\n","\n","print( \"\\nGPU: \")\n","print( C )\n","\n","C_diff = abs( C_cpu - C )\n","print( \"\\nDifference: \")\n","print( C_diff )\n","\n","if ( C_diff.all() < 10 ** -6 ):\n","    print( \"The result matrices are (nearly) the same.\" )   \n","else:\n","    print( \"The matrices are not the same. Something is wrong.\" )"]},{"cell_type":"markdown","metadata":{},"source":["Once again differences are appearing. Seems to be related to the ratio of the size of matrix and tile width?"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPpM2cfgi1eNvtNcnxiT2sO","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
